{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T19:39:50.622893500Z",
     "start_time": "2023-09-30T19:39:48.975900800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
      "C:\\Users\\bobco\\anaconda3\\envs\\ddpg\\lib\\site-packages\\gym\\envs\\registration.py:415: UserWarning: \u001B[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import wrappers\n",
    "from torch.autograd import Variable\n",
    "from collections import deque\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, max_size=1000000):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        \n",
    "    def add(self, transition):\n",
    "        if len(self.storage) >= self.max_size:\n",
    "            self.storage[int(self.ptr)] = transition\n",
    "            self.ptr = (self.ptr + 1) % self.max_size # This is a reset to set self.ptr back to 0 when it hits max size.\n",
    "        else:\n",
    "            self.storage.append(transition)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), batch_size)\n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "        for i in ind:\n",
    "            state, next_state, action, reward, done = self.storage[i]\n",
    "            batch_states.append(np.array(state, copy=False))\n",
    "            batch_next_states.append(np.array(next_state, copy=False))\n",
    "            batch_actions.append(np.array(action, copy=False))\n",
    "            batch_rewards.append(np.array(reward, copy=False))\n",
    "            batch_dones.append(np.array(done, copy=False))\n",
    "            \n",
    "        batch_states = np.array(batch_states)\n",
    "        batch_next_states = np.array(batch_next_states)\n",
    "        batch_actions = np.array(batch_actions)\n",
    "        batch_rewards = np.array(batch_rewards).reshape(-1, 1)\n",
    "        batch_dones = np.array(batch_dones)\n",
    "        \n",
    "        return batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones\n",
    "    \n",
    "    def can_sample(self, batch_size):\n",
    "        if len(self.storage) > batch_size * 10:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:39:50.638905900Z",
     "start_time": "2023-09-30T19:39:50.621894Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "        return x\n",
    "\n",
    "    def save_the_model(self, weights_filename='models/actor_latest.pt'):\n",
    "        # Take the default weights filename(latest.pt) and save it\n",
    "        torch.save(self.state_dict(), weights_filename)\n",
    "\n",
    "\n",
    "    def load_the_model(self, weights_filename='models/actor_latest.pt'):\n",
    "        try:\n",
    "            self.load_state_dict(torch.load(weights_filename))\n",
    "            print(f\"Successfully loaded weights file {weights_filename}\")\n",
    "        except:\n",
    "            print(f\"No weights file available at {weights_filename}\")\n",
    "    \n",
    "                "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:39:50.655932300Z",
     "start_time": "2023-09-30T19:39:50.637904800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        # First Critic Network.\n",
    "        self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, 1)\n",
    "        \n",
    "        # Second critic network\n",
    "        self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_5 = nn.Linear(400, 300)\n",
    "        self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x, u):\n",
    "        \n",
    "        \n",
    "        \n",
    "        xu = torch.cat([x, u], 1)\n",
    "        \n",
    "        # First critic forward prop\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        \n",
    "        # Second critic forward prop\n",
    "        x2 = F.relu(self.layer_4(xu))\n",
    "        x2 = F.relu(self.layer_5(x2))\n",
    "        x2 = self.layer_6(x2)\n",
    "        \n",
    "        return x1, x2\n",
    "    \n",
    "    def Q1(self, x, u):\n",
    "        \n",
    "        xu = torch.cat([x, u], 1)\n",
    "        \n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        return x1\n",
    "\n",
    "    def save_the_model(self, weights_filename='models/critic_latest.pt'):\n",
    "        # Take the default weights filename(latest.pt) and save it\n",
    "        torch.save(self.state_dict(), weights_filename)\n",
    "\n",
    "\n",
    "    def load_the_model(self, weights_filename='models/critic_latest.pt'):\n",
    "        try:\n",
    "            self.load_state_dict(torch.load(weights_filename))\n",
    "            print(f\"Successfully loaded weights file {weights_filename}\")\n",
    "        except:\n",
    "            print(f\"No weights file available at {weights_filename}\")\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:39:50.668036800Z",
     "start_time": "2023-09-30T19:39:50.654930800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:39:50.686174700Z",
     "start_time": "2023-09-30T19:39:50.669037700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action, device=None):\n",
    "        self.device = device\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(self.device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(self.device)\n",
    "        self.actor.load_the_model()\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "        \n",
    "        self.critic = Critic(state_dim, action_dim).to(self.device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(self.device)\n",
    "        self.critic.load_the_model()\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "        \n",
    "        self.max_action = max_action\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.Tensor(state.reshape(1, -1)).to(self.device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "    \n",
    "    def train(self, replay_buffer: ReplayBuffer, epochs, batch_size=100, discount=0.99, tau=0.005, \n",
    "              policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if replay_buffer.can_sample(batch_size):\n",
    "                batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size=100)\n",
    "                \n",
    "                state  = torch.Tensor(batch_states).to(self.device)\n",
    "                next_state  = torch.Tensor(batch_next_states).to(self.device)\n",
    "                action  = torch.Tensor(batch_actions).to(self.device)\n",
    "                reward  = torch.Tensor(batch_rewards).to(self.device)\n",
    "                done  = torch.Tensor(batch_dones).to(self.device)\n",
    "                \n",
    "                # Step 5: From the next state s', the actor target plays the next action a'\n",
    "                next_action = self.actor_target(next_state).to(self.device)\n",
    "                \n",
    "                # Step 6: Add Gaussian noise\n",
    "                noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(self.device)\n",
    "                noise = noise.clamp(-noise_clip, +noise_clip)\n",
    "                next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "                \n",
    "                # Step 7: Get critic q value\n",
    "                target_q1, target_q2 = self.critic_target(next_state, next_action)\n",
    "                                    \n",
    "                # Step 8: We keep the minimum of these two Q-values\n",
    "                target_q = torch.min(target_q1, target_q2)  \n",
    "                \n",
    "                # Step 9: We get the final target of the two Critic models, which is Qt = r + y * min(Qt1, Qt2), where y is the discount factor.\n",
    "                target_q = reward + ((1 - done) * discount * target_q).detach()\n",
    "                \n",
    "                # Step 10: The two critic models should take each the couple (s, a) as input and return two Q-Values(Q1 of s,a and Q2 of s,a) \n",
    "                current_q1, current_q2 = self.critic(state, action)\n",
    "\n",
    "                # Step 11                \n",
    "                critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "                \n",
    "                # Step 12: Compute the loss between the two critic models: Critic Loss = MSE_Loss(Q(s,a), Qt) + MSE_Loss(Q(s,a), Qt\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "                \n",
    "                # Step 13: Once every two iterations, update the actor model by performing gradient ascent on the output of the first critic model.\n",
    "                if epoch % policy_freq == 0:\n",
    "                    actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "                    self.actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    self.actor_optimizer.step()\n",
    "                    \n",
    "                    # Step 14: Still once every two iterations, use Polyak averaging to update the target weights\n",
    "                    for target_param, main_param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "                        target_param.data.copy_(tau * main_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "                    for target_param, main_param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "                        target_param.data.copy_(tau * main_param.data + (1.0 - tau) * target_param.data)\n",
    "                \n",
    "              # Making a save method to save a trained model\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "    \n",
    "    # Making a load method to load a pre-trained model\n",
    "    def load(self, filename, directory):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "\n",
    "                        \n",
    "                    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:39:50.698977900Z",
     "start_time": "2023-09-30T19:39:50.691172900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(np.array(obs))\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      avg_reward += reward\n",
    "          \n",
    "\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:39:50.717525200Z",
     "start_time": "2023-09-30T19:39:50.699983300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
    "seed = 0 # Random seed number\n",
    "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
    "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
    "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 100 # Size of the batch\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.005 # Target network update rate\n",
    "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated\n",
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:39:50.731089900Z",
     "start_time": "2023-09-30T19:39:50.716524100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./results\"):\n",
    "  os.makedirs(\"./results\")\n",
    "if save_models and not os.path.exists(\"./models\"):\n",
    "  os.makedirs(\"./models\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:39:50.748196Z",
     "start_time": "2023-09-30T19:39:50.731089900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bobco\\anaconda3\\envs\\ddpg\\lib\\site-packages\\pybullet_envs\\env_bases.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import parse_version\n",
      "C:\\Users\\bobco\\anaconda3\\envs\\ddpg\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:69: UserWarning: \u001B[33mWARN: Agent's minimum action space value is -infinity. This is probably too low.\u001B[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\bobco\\anaconda3\\envs\\ddpg\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:73: UserWarning: \u001B[33mWARN: Agent's maximum action space value is infinity. This is probably too high\u001B[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\bobco\\anaconda3\\envs\\ddpg\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001B[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:39:50.921098500Z",
     "start_time": "2023-09-30T19:39:50.748196Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:39:50.964380200Z",
     "start_time": "2023-09-30T19:39:50.920106400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bobco\\anaconda3\\envs\\ddpg\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:216: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. \u001B[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\bobco\\anaconda3\\envs\\ddpg\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:228: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001B[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\bobco\\anaconda3\\envs\\ddpg\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No weights file available at models/actor_latest.pt\n",
      "No weights file available at models/critic_latest.pt\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 9.807990\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "policy = TD3(state_dim, action_dim, max_action)\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "evaluations = [evaluate_policy(env, policy)]\n",
    "\n",
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "max_episode_steps = env._max_episode_steps\n",
    "save_env_vid = False\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:39:51.090342200Z",
     "start_time": "2023-09-30T19:39:50.936371200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "t0 = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:39:51.106382600Z",
     "start_time": "2023-09-30T19:39:51.090844400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 721 Episode Num: 1 Reward: 381.1608412336469\n",
      "Total Timesteps: 742 Episode Num: 2 Reward: 3.8410106292678385\n",
      "Total Timesteps: 1742 Episode Num: 3 Reward: 513.3346754733018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bobco\\AppData\\Local\\Temp\\ipykernel_4648\\1266979042.py:55: UserWarning: Using a target size (torch.Size([100, 100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 2076 Episode Num: 4 Reward: 182.53297325859165\n",
      "Total Timesteps: 3076 Episode Num: 5 Reward: 435.0931396498288\n",
      "Total Timesteps: 3222 Episode Num: 6 Reward: 65.37650211748844\n",
      "Total Timesteps: 4222 Episode Num: 7 Reward: 503.2689344251618\n",
      "Total Timesteps: 4632 Episode Num: 8 Reward: 187.77519935786697\n",
      "Total Timesteps: 4825 Episode Num: 9 Reward: 89.61508074495067\n",
      "Total Timesteps: 5825 Episode Num: 10 Reward: 480.918111548183\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 143.611270\n",
      "---------------------------------------\n",
      "Total Timesteps: 6825 Episode Num: 11 Reward: 525.7071220947724\n",
      "Total Timesteps: 7825 Episode Num: 12 Reward: 506.999570887151\n",
      "Total Timesteps: 8825 Episode Num: 13 Reward: 393.33442745288454\n",
      "Total Timesteps: 9346 Episode Num: 14 Reward: 241.2137639942901\n",
      "Total Timesteps: 10202 Episode Num: 15 Reward: 435.72408383395646\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 158.598200\n",
      "---------------------------------------\n",
      "Total Timesteps: 11202 Episode Num: 16 Reward: 148.14257868579259\n",
      "Total Timesteps: 12202 Episode Num: 17 Reward: 162.3580253011373\n",
      "Total Timesteps: 13202 Episode Num: 18 Reward: 301.853905025845\n",
      "Total Timesteps: 14202 Episode Num: 19 Reward: 147.64672267400633\n",
      "Total Timesteps: 15202 Episode Num: 20 Reward: 187.40580046199233\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 116.814856\n",
      "---------------------------------------\n",
      "Total Timesteps: 16202 Episode Num: 21 Reward: 93.62326554804287\n",
      "Total Timesteps: 17202 Episode Num: 22 Reward: 288.33693641242814\n",
      "Total Timesteps: 18202 Episode Num: 23 Reward: 70.58888758572262\n",
      "Total Timesteps: 19202 Episode Num: 24 Reward: 178.38089892418637\n",
      "Total Timesteps: 20202 Episode Num: 25 Reward: 188.6811313224898\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 137.304739\n",
      "---------------------------------------\n",
      "Total Timesteps: 21202 Episode Num: 26 Reward: 141.62032688262207\n",
      "Total Timesteps: 21392 Episode Num: 27 Reward: 20.172762132206447\n",
      "Total Timesteps: 22392 Episode Num: 28 Reward: 121.98829867463556\n",
      "Total Timesteps: 23392 Episode Num: 29 Reward: 257.7095702988969\n",
      "Total Timesteps: 24392 Episode Num: 30 Reward: 460.6052451466467\n",
      "Total Timesteps: 25392 Episode Num: 31 Reward: 423.60720527856716\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 380.611247\n",
      "---------------------------------------\n",
      "Total Timesteps: 26392 Episode Num: 32 Reward: 331.76951748109354\n",
      "Total Timesteps: 27392 Episode Num: 33 Reward: 441.24938793774635\n",
      "Total Timesteps: 28392 Episode Num: 34 Reward: 291.97445585799187\n",
      "Total Timesteps: 29392 Episode Num: 35 Reward: 177.1357691728231\n",
      "Total Timesteps: 30392 Episode Num: 36 Reward: 472.7474755614738\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -0.569666\n",
      "---------------------------------------\n",
      "Total Timesteps: 30412 Episode Num: 37 Reward: 0.1120006800616471\n",
      "Total Timesteps: 30432 Episode Num: 38 Reward: -0.3283259267137635\n",
      "Total Timesteps: 30452 Episode Num: 39 Reward: -0.24812410424914777\n",
      "Total Timesteps: 30472 Episode Num: 40 Reward: 0.6955619591429147\n",
      "Total Timesteps: 30493 Episode Num: 41 Reward: 0.4231526308880953\n",
      "Total Timesteps: 30606 Episode Num: 42 Reward: 23.530814589724933\n",
      "Total Timesteps: 31606 Episode Num: 43 Reward: 128.27807514805963\n",
      "Total Timesteps: 31711 Episode Num: 44 Reward: 9.413815758935247\n",
      "Total Timesteps: 32711 Episode Num: 45 Reward: 460.2723758791792\n",
      "Total Timesteps: 32834 Episode Num: 46 Reward: 41.673042904994595\n",
      "Total Timesteps: 33834 Episode Num: 47 Reward: 440.9336853462184\n",
      "Total Timesteps: 34834 Episode Num: 48 Reward: 554.4957740037256\n",
      "Total Timesteps: 34937 Episode Num: 49 Reward: 84.48857560575547\n",
      "Total Timesteps: 35937 Episode Num: 50 Reward: 625.4538810862487\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 575.143510\n",
      "---------------------------------------\n",
      "Total Timesteps: 36937 Episode Num: 51 Reward: 582.5872574215252\n",
      "Total Timesteps: 37937 Episode Num: 52 Reward: 606.347793982747\n",
      "Total Timesteps: 38905 Episode Num: 53 Reward: 446.23766257274013\n",
      "Total Timesteps: 39905 Episode Num: 54 Reward: 510.47844606620174\n",
      "Total Timesteps: 40905 Episode Num: 55 Reward: 697.8311847294185\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 503.224323\n",
      "---------------------------------------\n",
      "Total Timesteps: 41905 Episode Num: 56 Reward: 333.96866340398014\n",
      "Total Timesteps: 42905 Episode Num: 57 Reward: 763.5229921836045\n",
      "Total Timesteps: 43905 Episode Num: 58 Reward: 430.55323098098086\n",
      "Total Timesteps: 44905 Episode Num: 59 Reward: 333.2859360196254\n",
      "Total Timesteps: 45905 Episode Num: 60 Reward: 539.4674432181307\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 128.292183\n",
      "---------------------------------------\n",
      "Total Timesteps: 45994 Episode Num: 61 Reward: 53.216905881013076\n",
      "Total Timesteps: 46994 Episode Num: 62 Reward: 427.72379831420807\n",
      "Total Timesteps: 47994 Episode Num: 63 Reward: 412.9886695726562\n",
      "Total Timesteps: 48994 Episode Num: 64 Reward: 158.19583906445573\n",
      "Total Timesteps: 49994 Episode Num: 65 Reward: 603.0647851948515\n",
      "Total Timesteps: 50994 Episode Num: 66 Reward: 513.1231675011279\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 397.187393\n",
      "---------------------------------------\n",
      "Total Timesteps: 51994 Episode Num: 67 Reward: 546.0852648225725\n",
      "Total Timesteps: 52994 Episode Num: 68 Reward: 485.91395068554135\n",
      "Total Timesteps: 53994 Episode Num: 69 Reward: 260.66938590683304\n",
      "Total Timesteps: 54994 Episode Num: 70 Reward: 212.67086221656248\n",
      "Total Timesteps: 55994 Episode Num: 71 Reward: 503.84806645084694\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 572.165903\n",
      "---------------------------------------\n",
      "Total Timesteps: 56994 Episode Num: 72 Reward: 468.51217366855536\n",
      "Total Timesteps: 57994 Episode Num: 73 Reward: 353.0685763537552\n",
      "Total Timesteps: 58994 Episode Num: 74 Reward: 328.97471832242485\n",
      "Total Timesteps: 59994 Episode Num: 75 Reward: 265.4589089784008\n",
      "Total Timesteps: 60994 Episode Num: 76 Reward: 491.3499305354787\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 519.701612\n",
      "---------------------------------------\n",
      "Total Timesteps: 61994 Episode Num: 77 Reward: 593.972661407309\n",
      "Total Timesteps: 62994 Episode Num: 78 Reward: 487.3654080854691\n",
      "Total Timesteps: 63994 Episode Num: 79 Reward: 359.76198146009637\n",
      "Total Timesteps: 64994 Episode Num: 80 Reward: 606.437330653697\n",
      "Total Timesteps: 65994 Episode Num: 81 Reward: 438.5565919331915\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 346.710434\n",
      "---------------------------------------\n",
      "Total Timesteps: 66994 Episode Num: 82 Reward: 265.48340703110875\n",
      "Total Timesteps: 67994 Episode Num: 83 Reward: 306.86344639016517\n",
      "Total Timesteps: 68172 Episode Num: 84 Reward: 98.02547780364095\n",
      "Total Timesteps: 69172 Episode Num: 85 Reward: 400.3261081843022\n",
      "Total Timesteps: 70172 Episode Num: 86 Reward: 227.50194843349777\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 236.320989\n",
      "---------------------------------------\n",
      "Total Timesteps: 71172 Episode Num: 87 Reward: 256.1270023214094\n",
      "Total Timesteps: 72172 Episode Num: 88 Reward: 234.45333043466064\n",
      "Total Timesteps: 73172 Episode Num: 89 Reward: 236.61524702210122\n",
      "Total Timesteps: 74172 Episode Num: 90 Reward: 302.3153472792196\n",
      "Total Timesteps: 75172 Episode Num: 91 Reward: 294.3949888519468\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 229.823367\n",
      "---------------------------------------\n",
      "Total Timesteps: 75333 Episode Num: 92 Reward: 103.55037568809921\n",
      "Total Timesteps: 76173 Episode Num: 93 Reward: 521.2356406111544\n",
      "Total Timesteps: 76378 Episode Num: 94 Reward: 101.6949164921318\n",
      "Total Timesteps: 77378 Episode Num: 95 Reward: 253.88129571493405\n",
      "Total Timesteps: 77819 Episode Num: 96 Reward: 199.42438870468754\n",
      "Total Timesteps: 78819 Episode Num: 97 Reward: 176.41777218832203\n",
      "Total Timesteps: 79819 Episode Num: 98 Reward: 310.0125582790184\n",
      "Total Timesteps: 80819 Episode Num: 99 Reward: 363.761621842188\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 551.633247\n",
      "---------------------------------------\n",
      "Total Timesteps: 81819 Episode Num: 100 Reward: 612.6849490941121\n",
      "Total Timesteps: 82303 Episode Num: 101 Reward: 285.6353295946259\n",
      "Total Timesteps: 83303 Episode Num: 102 Reward: 518.0941222450838\n",
      "Total Timesteps: 84303 Episode Num: 103 Reward: 405.8121046066964\n",
      "Total Timesteps: 85303 Episode Num: 104 Reward: 274.83122449555776\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 334.110182\n",
      "---------------------------------------\n",
      "Total Timesteps: 86303 Episode Num: 105 Reward: 549.1145407025767\n",
      "Total Timesteps: 87303 Episode Num: 106 Reward: 461.2910553893196\n",
      "Total Timesteps: 88303 Episode Num: 107 Reward: 582.6371010283556\n",
      "Total Timesteps: 89303 Episode Num: 108 Reward: 761.3889232360569\n",
      "Total Timesteps: 89985 Episode Num: 109 Reward: 467.4981383691084\n",
      "Total Timesteps: 90985 Episode Num: 110 Reward: 562.9411971907127\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 554.674812\n",
      "---------------------------------------\n",
      "Total Timesteps: 91985 Episode Num: 111 Reward: 517.2101348249772\n",
      "Total Timesteps: 92985 Episode Num: 112 Reward: 645.5282779794591\n",
      "Total Timesteps: 93985 Episode Num: 113 Reward: 469.1216878057973\n",
      "Total Timesteps: 94985 Episode Num: 114 Reward: 641.4149325299679\n",
      "Total Timesteps: 95985 Episode Num: 115 Reward: 633.4424084091978\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 641.049135\n",
      "---------------------------------------\n",
      "Total Timesteps: 96985 Episode Num: 116 Reward: 602.951316475926\n",
      "Total Timesteps: 97985 Episode Num: 117 Reward: 636.8833129167084\n",
      "Total Timesteps: 98985 Episode Num: 118 Reward: 533.6714173011451\n",
      "Total Timesteps: 99985 Episode Num: 119 Reward: 523.0667306407807\n",
      "Total Timesteps: 100985 Episode Num: 120 Reward: 609.4992240097567\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 651.197474\n",
      "---------------------------------------\n",
      "Total Timesteps: 101985 Episode Num: 121 Reward: 551.5349216295742\n",
      "Total Timesteps: 102985 Episode Num: 122 Reward: 674.8549957559989\n",
      "Total Timesteps: 103985 Episode Num: 123 Reward: 684.8387847008349\n",
      "Total Timesteps: 104985 Episode Num: 124 Reward: 743.5037855919587\n",
      "Total Timesteps: 105985 Episode Num: 125 Reward: 963.1240866253343\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 918.503497\n",
      "---------------------------------------\n",
      "Total Timesteps: 106985 Episode Num: 126 Reward: 961.8954314775366\n",
      "Total Timesteps: 107985 Episode Num: 127 Reward: 828.5983555167425\n",
      "Total Timesteps: 108985 Episode Num: 128 Reward: 673.8910547532064\n",
      "Total Timesteps: 109985 Episode Num: 129 Reward: 802.5666302179383\n",
      "Total Timesteps: 110985 Episode Num: 130 Reward: 675.8836486892366\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 774.107480\n",
      "---------------------------------------\n",
      "Total Timesteps: 111985 Episode Num: 131 Reward: 858.4269056936513\n",
      "Total Timesteps: 112985 Episode Num: 132 Reward: 727.7829699740881\n",
      "Total Timesteps: 113985 Episode Num: 133 Reward: 533.8271539404125\n",
      "Total Timesteps: 114985 Episode Num: 134 Reward: 578.4657701260892\n",
      "Total Timesteps: 115985 Episode Num: 135 Reward: 917.3464810992508\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 735.269026\n",
      "---------------------------------------\n",
      "Total Timesteps: 116985 Episode Num: 136 Reward: 583.1609713554959\n",
      "Total Timesteps: 117985 Episode Num: 137 Reward: 878.224203562622\n",
      "Total Timesteps: 118985 Episode Num: 138 Reward: 653.1435820326564\n",
      "Total Timesteps: 119985 Episode Num: 139 Reward: 441.92322918500196\n",
      "Total Timesteps: 120985 Episode Num: 140 Reward: 866.3819320914974\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 720.736432\n",
      "---------------------------------------\n",
      "Total Timesteps: 121985 Episode Num: 141 Reward: 649.9721890550253\n",
      "Total Timesteps: 122985 Episode Num: 142 Reward: 923.0631274469259\n",
      "Total Timesteps: 123985 Episode Num: 143 Reward: 797.4585289817584\n",
      "Total Timesteps: 124985 Episode Num: 144 Reward: 541.592704018143\n",
      "Total Timesteps: 125985 Episode Num: 145 Reward: 412.3215073062211\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 638.970975\n",
      "---------------------------------------\n",
      "Total Timesteps: 126985 Episode Num: 146 Reward: 786.7471960134915\n",
      "Total Timesteps: 127985 Episode Num: 147 Reward: 273.71333502462954\n",
      "Total Timesteps: 128158 Episode Num: 148 Reward: 17.04581513260889\n",
      "Total Timesteps: 128817 Episode Num: 149 Reward: 493.57572678304615\n",
      "Total Timesteps: 129817 Episode Num: 150 Reward: 707.0901506489579\n",
      "Total Timesteps: 130817 Episode Num: 151 Reward: 1003.8602088925437\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 968.036517\n",
      "---------------------------------------\n",
      "Total Timesteps: 131817 Episode Num: 152 Reward: 1092.268357817181\n",
      "Total Timesteps: 132817 Episode Num: 153 Reward: 981.721671042505\n",
      "Total Timesteps: 133817 Episode Num: 154 Reward: 1009.5457011924797\n",
      "Total Timesteps: 134817 Episode Num: 155 Reward: 1090.5955445019883\n",
      "Total Timesteps: 135276 Episode Num: 156 Reward: 436.5892763765099\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 917.739075\n",
      "---------------------------------------\n",
      "Total Timesteps: 136276 Episode Num: 157 Reward: 1217.2214846553932\n",
      "Total Timesteps: 137276 Episode Num: 158 Reward: 622.7035829578433\n",
      "Total Timesteps: 138276 Episode Num: 159 Reward: 963.7913200733096\n",
      "Total Timesteps: 139276 Episode Num: 160 Reward: 953.3349388408451\n",
      "Total Timesteps: 139805 Episode Num: 161 Reward: 468.12358356119324\n",
      "Total Timesteps: 140805 Episode Num: 162 Reward: 985.9127299180416\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 973.076804\n",
      "---------------------------------------\n",
      "Total Timesteps: 141805 Episode Num: 163 Reward: 996.4163700799164\n",
      "Total Timesteps: 142805 Episode Num: 164 Reward: 568.6755341363407\n",
      "Total Timesteps: 143805 Episode Num: 165 Reward: 1120.2614349543344\n",
      "Total Timesteps: 144805 Episode Num: 166 Reward: 743.4352332420118\n",
      "Total Timesteps: 145805 Episode Num: 167 Reward: 1266.7681488124815\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1177.497038\n",
      "---------------------------------------\n",
      "Total Timesteps: 146805 Episode Num: 168 Reward: 1150.3952228327598\n",
      "Total Timesteps: 147805 Episode Num: 169 Reward: 1329.9265240351428\n",
      "Total Timesteps: 148805 Episode Num: 170 Reward: 1136.4763593729328\n",
      "Total Timesteps: 149805 Episode Num: 171 Reward: 814.095785356392\n",
      "Total Timesteps: 150165 Episode Num: 172 Reward: 343.12485265114327\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1096.334575\n",
      "---------------------------------------\n",
      "Total Timesteps: 151165 Episode Num: 173 Reward: 623.9544711736368\n",
      "Total Timesteps: 151604 Episode Num: 174 Reward: 469.5979914728538\n",
      "Total Timesteps: 152604 Episode Num: 175 Reward: 1158.8534622197587\n",
      "Total Timesteps: 153604 Episode Num: 176 Reward: 1111.7072979971363\n",
      "Total Timesteps: 154604 Episode Num: 177 Reward: 632.1025950427571\n",
      "Total Timesteps: 155604 Episode Num: 178 Reward: 1519.3845993268046\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1086.504816\n",
      "---------------------------------------\n",
      "Total Timesteps: 156604 Episode Num: 179 Reward: 1181.5496866832227\n",
      "Total Timesteps: 157604 Episode Num: 180 Reward: 1205.9103768349864\n",
      "Total Timesteps: 158604 Episode Num: 181 Reward: 1331.9000269481603\n",
      "Total Timesteps: 159604 Episode Num: 182 Reward: 1089.1823092348213\n",
      "Total Timesteps: 160604 Episode Num: 183 Reward: 1304.4662044629222\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1413.905723\n",
      "---------------------------------------\n",
      "Total Timesteps: 161604 Episode Num: 184 Reward: 1414.420628973239\n",
      "Total Timesteps: 162604 Episode Num: 185 Reward: 1475.6607239681193\n",
      "Total Timesteps: 163604 Episode Num: 186 Reward: 1480.939169781441\n",
      "Total Timesteps: 164604 Episode Num: 187 Reward: 857.5559966210005\n",
      "Total Timesteps: 165604 Episode Num: 188 Reward: 1732.2039776176243\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1623.057975\n",
      "---------------------------------------\n",
      "Total Timesteps: 166604 Episode Num: 189 Reward: 1531.8758666308859\n",
      "Total Timesteps: 167604 Episode Num: 190 Reward: 1402.9601607956158\n",
      "Total Timesteps: 168604 Episode Num: 191 Reward: 1529.3682142858354\n",
      "Total Timesteps: 169604 Episode Num: 192 Reward: 1725.2485275298352\n",
      "Total Timesteps: 170604 Episode Num: 193 Reward: 663.3176763970589\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1641.582546\n",
      "---------------------------------------\n",
      "Total Timesteps: 171604 Episode Num: 194 Reward: 1708.61824909476\n",
      "Total Timesteps: 172604 Episode Num: 195 Reward: 1096.1824367763177\n",
      "Total Timesteps: 173604 Episode Num: 196 Reward: 1702.4756190919004\n",
      "Total Timesteps: 174604 Episode Num: 197 Reward: 1531.8600816137189\n",
      "Total Timesteps: 175604 Episode Num: 198 Reward: 1620.6813796472566\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1686.812445\n",
      "---------------------------------------\n",
      "Total Timesteps: 176604 Episode Num: 199 Reward: 1656.4741592132186\n",
      "Total Timesteps: 177604 Episode Num: 200 Reward: 1751.9545865164914\n",
      "Total Timesteps: 178604 Episode Num: 201 Reward: 1764.0021123746694\n",
      "Total Timesteps: 179604 Episode Num: 202 Reward: 1656.6412682165014\n",
      "Total Timesteps: 180604 Episode Num: 203 Reward: 1590.2214551128131\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1626.632610\n",
      "---------------------------------------\n",
      "Total Timesteps: 181604 Episode Num: 204 Reward: 1653.9046313768838\n",
      "Total Timesteps: 182604 Episode Num: 205 Reward: 1581.9408050374293\n",
      "Total Timesteps: 183604 Episode Num: 206 Reward: 1648.1517507462684\n",
      "Total Timesteps: 184604 Episode Num: 207 Reward: 1594.6163953607124\n",
      "Total Timesteps: 185604 Episode Num: 208 Reward: 1579.1745067634215\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1699.771586\n",
      "---------------------------------------\n",
      "Total Timesteps: 186604 Episode Num: 209 Reward: 1811.6823905502124\n",
      "Total Timesteps: 187604 Episode Num: 210 Reward: 1856.50423755085\n",
      "Total Timesteps: 188604 Episode Num: 211 Reward: 1797.7432463982582\n",
      "Total Timesteps: 189604 Episode Num: 212 Reward: 1907.6397743977202\n",
      "Total Timesteps: 190604 Episode Num: 213 Reward: 1942.839994977717\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1737.526060\n",
      "---------------------------------------\n",
      "Total Timesteps: 191604 Episode Num: 214 Reward: 1710.4863129620585\n",
      "Total Timesteps: 192604 Episode Num: 215 Reward: 1853.540624834597\n",
      "Total Timesteps: 193604 Episode Num: 216 Reward: 1826.2865122908402\n",
      "Total Timesteps: 194604 Episode Num: 217 Reward: 1996.5007353633412\n",
      "Total Timesteps: 195604 Episode Num: 218 Reward: 1702.125754023445\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1684.472566\n",
      "---------------------------------------\n",
      "Total Timesteps: 196604 Episode Num: 219 Reward: 1691.9261049287275\n",
      "Total Timesteps: 197604 Episode Num: 220 Reward: 1904.934519809061\n",
      "Total Timesteps: 198604 Episode Num: 221 Reward: 1772.5828248351145\n",
      "Total Timesteps: 199604 Episode Num: 222 Reward: 1495.4721616402155\n",
      "Total Timesteps: 200604 Episode Num: 223 Reward: 1996.988572850984\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1883.343557\n",
      "---------------------------------------\n",
      "Total Timesteps: 201604 Episode Num: 224 Reward: 1910.1261238159084\n",
      "Total Timesteps: 202604 Episode Num: 225 Reward: 1791.5499143899222\n",
      "Total Timesteps: 203604 Episode Num: 226 Reward: 2038.4132039611154\n",
      "Total Timesteps: 204604 Episode Num: 227 Reward: 2039.1493894699463\n",
      "Total Timesteps: 205604 Episode Num: 228 Reward: 1846.6053450952036\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1898.996810\n",
      "---------------------------------------\n",
      "Total Timesteps: 206604 Episode Num: 229 Reward: 1827.7466467385464\n",
      "Total Timesteps: 207604 Episode Num: 230 Reward: 1986.234999349753\n",
      "Total Timesteps: 208604 Episode Num: 231 Reward: 1846.1838325179906\n",
      "Total Timesteps: 209604 Episode Num: 232 Reward: 2017.4137672159347\n",
      "Total Timesteps: 210604 Episode Num: 233 Reward: 2024.6651776972565\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2053.384567\n",
      "---------------------------------------\n",
      "Total Timesteps: 211604 Episode Num: 234 Reward: 2028.5282318989719\n",
      "Total Timesteps: 212604 Episode Num: 235 Reward: 1910.8533668141758\n",
      "Total Timesteps: 213604 Episode Num: 236 Reward: 1895.344776291157\n",
      "Total Timesteps: 214604 Episode Num: 237 Reward: 1901.822610251083\n",
      "Total Timesteps: 215604 Episode Num: 238 Reward: 2018.6736171667462\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1973.837655\n",
      "---------------------------------------\n",
      "Total Timesteps: 216604 Episode Num: 239 Reward: 1915.869354855451\n",
      "Total Timesteps: 217604 Episode Num: 240 Reward: 1982.9730946756342\n",
      "Total Timesteps: 218604 Episode Num: 241 Reward: 1901.8632836623856\n",
      "Total Timesteps: 219604 Episode Num: 242 Reward: 1947.5640350332499\n",
      "Total Timesteps: 220604 Episode Num: 243 Reward: 1712.40246695048\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2041.059948\n",
      "---------------------------------------\n",
      "Total Timesteps: 221604 Episode Num: 244 Reward: 1985.7910404414997\n",
      "Total Timesteps: 222604 Episode Num: 245 Reward: 2002.1664074480943\n",
      "Total Timesteps: 223604 Episode Num: 246 Reward: 2075.153590901257\n",
      "Total Timesteps: 224604 Episode Num: 247 Reward: 1972.5769214782383\n",
      "Total Timesteps: 225604 Episode Num: 248 Reward: 1958.359610593072\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2174.708185\n",
      "---------------------------------------\n",
      "Total Timesteps: 226604 Episode Num: 249 Reward: 2120.2324674979304\n",
      "Total Timesteps: 227604 Episode Num: 250 Reward: 2072.5879504636196\n",
      "Total Timesteps: 228604 Episode Num: 251 Reward: 2085.255300511692\n",
      "Total Timesteps: 229604 Episode Num: 252 Reward: 2047.677490172053\n",
      "Total Timesteps: 230604 Episode Num: 253 Reward: 2024.8318082690764\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1140.849180\n",
      "---------------------------------------\n",
      "Total Timesteps: 231604 Episode Num: 254 Reward: 285.6155445461098\n",
      "Total Timesteps: 232604 Episode Num: 255 Reward: 2108.937691501818\n",
      "Total Timesteps: 233604 Episode Num: 256 Reward: 2016.464157048313\n",
      "Total Timesteps: 234604 Episode Num: 257 Reward: 1977.7386762147544\n",
      "Total Timesteps: 235604 Episode Num: 258 Reward: 2027.5858097947362\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2166.085428\n",
      "---------------------------------------\n",
      "Total Timesteps: 236604 Episode Num: 259 Reward: 2120.747117793673\n",
      "Total Timesteps: 237604 Episode Num: 260 Reward: 2048.271047142845\n",
      "Total Timesteps: 238604 Episode Num: 261 Reward: 2080.52428761067\n",
      "Total Timesteps: 239604 Episode Num: 262 Reward: 1896.546418792207\n",
      "Total Timesteps: 240604 Episode Num: 263 Reward: 2150.4240945101305\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2138.978279\n",
      "---------------------------------------\n",
      "Total Timesteps: 241604 Episode Num: 264 Reward: 2114.64175702154\n",
      "Total Timesteps: 242604 Episode Num: 265 Reward: 1844.7036253984174\n",
      "Total Timesteps: 243604 Episode Num: 266 Reward: 2163.821365046615\n",
      "Total Timesteps: 244604 Episode Num: 267 Reward: 2219.171414422842\n",
      "Total Timesteps: 245604 Episode Num: 268 Reward: 1993.5139606440173\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2098.781555\n",
      "---------------------------------------\n",
      "Total Timesteps: 246604 Episode Num: 269 Reward: 2033.3698226109275\n",
      "Total Timesteps: 247604 Episode Num: 270 Reward: 2051.6261909956124\n",
      "Total Timesteps: 248604 Episode Num: 271 Reward: 2072.872099016535\n",
      "Total Timesteps: 249604 Episode Num: 272 Reward: 2100.907351995725\n",
      "Total Timesteps: 250604 Episode Num: 273 Reward: 2138.4159538359236\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2143.068303\n",
      "---------------------------------------\n",
      "Total Timesteps: 251604 Episode Num: 274 Reward: 2073.665106942167\n",
      "Total Timesteps: 252604 Episode Num: 275 Reward: 2070.532332893539\n",
      "Total Timesteps: 253604 Episode Num: 276 Reward: 2100.2196808525596\n",
      "Total Timesteps: 254604 Episode Num: 277 Reward: 2090.6036674539146\n",
      "Total Timesteps: 255604 Episode Num: 278 Reward: 2158.9929416283635\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2067.583975\n",
      "---------------------------------------\n",
      "Total Timesteps: 256604 Episode Num: 279 Reward: 2057.7989949916328\n",
      "Total Timesteps: 257604 Episode Num: 280 Reward: 2144.0791400612075\n",
      "Total Timesteps: 258604 Episode Num: 281 Reward: 2152.114608602829\n",
      "Total Timesteps: 259604 Episode Num: 282 Reward: 2024.5795345976242\n",
      "Total Timesteps: 260604 Episode Num: 283 Reward: 2193.2392758933697\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2097.432664\n",
      "---------------------------------------\n",
      "Total Timesteps: 261604 Episode Num: 284 Reward: 2052.1722045684214\n",
      "Total Timesteps: 262604 Episode Num: 285 Reward: 2094.171552745008\n",
      "Total Timesteps: 263604 Episode Num: 286 Reward: 2187.2026213800586\n",
      "Total Timesteps: 264604 Episode Num: 287 Reward: 2185.4379003030595\n",
      "Total Timesteps: 265604 Episode Num: 288 Reward: 2202.1178570042894\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2241.422732\n",
      "---------------------------------------\n",
      "Total Timesteps: 266604 Episode Num: 289 Reward: 2084.701173639144\n",
      "Total Timesteps: 267604 Episode Num: 290 Reward: 2121.52023112978\n",
      "Total Timesteps: 268604 Episode Num: 291 Reward: 1889.6168248068589\n",
      "Total Timesteps: 269604 Episode Num: 292 Reward: 2036.8101365310247\n",
      "Total Timesteps: 270604 Episode Num: 293 Reward: 2271.906803118681\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2227.043516\n",
      "---------------------------------------\n",
      "Total Timesteps: 271604 Episode Num: 294 Reward: 2206.9637196652525\n",
      "Total Timesteps: 272604 Episode Num: 295 Reward: 2194.522676799968\n",
      "Total Timesteps: 273604 Episode Num: 296 Reward: 2190.595856882844\n",
      "Total Timesteps: 274604 Episode Num: 297 Reward: 2196.436370285494\n",
      "Total Timesteps: 275604 Episode Num: 298 Reward: 2125.214000342306\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2204.378008\n",
      "---------------------------------------\n",
      "Total Timesteps: 276604 Episode Num: 299 Reward: 2141.0138138808347\n",
      "Total Timesteps: 277604 Episode Num: 300 Reward: 2221.504839920042\n",
      "Total Timesteps: 278604 Episode Num: 301 Reward: 2226.770129964489\n",
      "Total Timesteps: 279604 Episode Num: 302 Reward: 2296.95247575412\n",
      "Total Timesteps: 280604 Episode Num: 303 Reward: 2272.3143181452356\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2234.980647\n",
      "---------------------------------------\n",
      "Total Timesteps: 281604 Episode Num: 304 Reward: 2172.180802973423\n",
      "Total Timesteps: 282604 Episode Num: 305 Reward: 2378.733152502972\n",
      "Total Timesteps: 283604 Episode Num: 306 Reward: 2241.197985470833\n",
      "Total Timesteps: 284604 Episode Num: 307 Reward: 2379.2541472255107\n",
      "Total Timesteps: 285604 Episode Num: 308 Reward: 2384.000073095827\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2310.480983\n",
      "---------------------------------------\n",
      "Total Timesteps: 286604 Episode Num: 309 Reward: 2338.452788573488\n",
      "Total Timesteps: 287604 Episode Num: 310 Reward: 2319.0340609575796\n",
      "Total Timesteps: 288604 Episode Num: 311 Reward: 2278.210951650418\n",
      "Total Timesteps: 289604 Episode Num: 312 Reward: 2321.3128226721165\n",
      "Total Timesteps: 290604 Episode Num: 313 Reward: 2270.8955256011654\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2305.422747\n",
      "---------------------------------------\n",
      "Total Timesteps: 291604 Episode Num: 314 Reward: 2266.839382158946\n",
      "Total Timesteps: 292604 Episode Num: 315 Reward: 2325.8919079097586\n",
      "Total Timesteps: 293604 Episode Num: 316 Reward: 2241.0950453602154\n",
      "Total Timesteps: 294604 Episode Num: 317 Reward: 2403.9936400701913\n",
      "Total Timesteps: 295604 Episode Num: 318 Reward: 2157.4445792060255\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2315.449331\n",
      "---------------------------------------\n",
      "Total Timesteps: 296604 Episode Num: 319 Reward: 2268.7891806547323\n",
      "Total Timesteps: 297604 Episode Num: 320 Reward: 2254.5041635113666\n",
      "Total Timesteps: 298604 Episode Num: 321 Reward: 2356.842125417638\n",
      "Total Timesteps: 299604 Episode Num: 322 Reward: 2251.569853638537\n",
      "Total Timesteps: 300604 Episode Num: 323 Reward: 2398.153350281854\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2451.724409\n",
      "---------------------------------------\n",
      "Total Timesteps: 301604 Episode Num: 324 Reward: 2418.9336685416283\n",
      "Total Timesteps: 302604 Episode Num: 325 Reward: 2430.2293981777793\n",
      "Total Timesteps: 303604 Episode Num: 326 Reward: 2279.7273662029083\n",
      "Total Timesteps: 304604 Episode Num: 327 Reward: 2281.3513401109517\n",
      "Total Timesteps: 305604 Episode Num: 328 Reward: 2414.100237213602\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2471.294198\n",
      "---------------------------------------\n",
      "Total Timesteps: 306604 Episode Num: 329 Reward: 2447.5611352600263\n",
      "Total Timesteps: 307604 Episode Num: 330 Reward: 2230.640282161318\n",
      "Total Timesteps: 308604 Episode Num: 331 Reward: 2317.8887067990813\n",
      "Total Timesteps: 309604 Episode Num: 332 Reward: 2343.051455615744\n",
      "Total Timesteps: 310604 Episode Num: 333 Reward: 2370.0207631354665\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2368.038124\n",
      "---------------------------------------\n",
      "Total Timesteps: 311604 Episode Num: 334 Reward: 2320.150156345328\n",
      "Total Timesteps: 312604 Episode Num: 335 Reward: 2292.562222485155\n",
      "Total Timesteps: 313604 Episode Num: 336 Reward: 2379.1681717966103\n",
      "Total Timesteps: 314604 Episode Num: 337 Reward: 2461.783765357242\n",
      "Total Timesteps: 315604 Episode Num: 338 Reward: 2507.910647357174\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2304.440914\n",
      "---------------------------------------\n",
      "Total Timesteps: 316604 Episode Num: 339 Reward: 2226.095915785973\n",
      "Total Timesteps: 317604 Episode Num: 340 Reward: 2536.1393759419975\n",
      "Total Timesteps: 318604 Episode Num: 341 Reward: 2530.9894066035645\n",
      "Total Timesteps: 319604 Episode Num: 342 Reward: 2373.119446399579\n",
      "Total Timesteps: 320604 Episode Num: 343 Reward: 2341.9312566568033\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2497.019866\n",
      "---------------------------------------\n",
      "Total Timesteps: 321604 Episode Num: 344 Reward: 2492.9403155315017\n",
      "Total Timesteps: 322604 Episode Num: 345 Reward: 2510.744141268141\n",
      "Total Timesteps: 323604 Episode Num: 346 Reward: 2452.720412412395\n",
      "Total Timesteps: 324604 Episode Num: 347 Reward: 2376.732059520508\n",
      "Total Timesteps: 325604 Episode Num: 348 Reward: 2517.4654408558354\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2454.453294\n",
      "---------------------------------------\n",
      "Total Timesteps: 326604 Episode Num: 349 Reward: 2382.4805932619993\n",
      "Total Timesteps: 327604 Episode Num: 350 Reward: 2551.8931635417566\n",
      "Total Timesteps: 328604 Episode Num: 351 Reward: 2362.8196935738397\n",
      "Total Timesteps: 329604 Episode Num: 352 Reward: 2524.522599029182\n",
      "Total Timesteps: 330604 Episode Num: 353 Reward: 2530.968011144705\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2449.079123\n",
      "---------------------------------------\n",
      "Total Timesteps: 331604 Episode Num: 354 Reward: 2400.4951868357184\n",
      "Total Timesteps: 332604 Episode Num: 355 Reward: 2500.7058490213913\n",
      "Total Timesteps: 333604 Episode Num: 356 Reward: 2553.4868066071876\n",
      "Total Timesteps: 334604 Episode Num: 357 Reward: 2368.6730795499357\n",
      "Total Timesteps: 335604 Episode Num: 358 Reward: 2544.704305196048\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2502.541124\n",
      "---------------------------------------\n",
      "Total Timesteps: 336604 Episode Num: 359 Reward: 2438.9033888862264\n",
      "Total Timesteps: 337604 Episode Num: 360 Reward: 2168.1864140786543\n",
      "Total Timesteps: 338604 Episode Num: 361 Reward: 2152.3288243880984\n",
      "Total Timesteps: 339604 Episode Num: 362 Reward: 2453.348929439867\n",
      "Total Timesteps: 340604 Episode Num: 363 Reward: 2403.2872044084897\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2611.112593\n",
      "---------------------------------------\n",
      "Total Timesteps: 341604 Episode Num: 364 Reward: 2590.5642370502187\n",
      "Total Timesteps: 342604 Episode Num: 365 Reward: 2398.0222842766416\n",
      "Total Timesteps: 343604 Episode Num: 366 Reward: 2360.1851213310615\n",
      "Total Timesteps: 344604 Episode Num: 367 Reward: 2464.2872318204186\n",
      "Total Timesteps: 345604 Episode Num: 368 Reward: 2432.9101507179953\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2472.368388\n",
      "---------------------------------------\n",
      "Total Timesteps: 346604 Episode Num: 369 Reward: 2468.1636509084783\n",
      "Total Timesteps: 347604 Episode Num: 370 Reward: 2356.0805166088926\n",
      "Total Timesteps: 348604 Episode Num: 371 Reward: 2468.4805297530443\n",
      "Total Timesteps: 349604 Episode Num: 372 Reward: 2407.8414120387606\n",
      "Total Timesteps: 350604 Episode Num: 373 Reward: 2413.0279669992806\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2470.005920\n",
      "---------------------------------------\n",
      "Total Timesteps: 351604 Episode Num: 374 Reward: 2457.2858947684304\n",
      "Total Timesteps: 352604 Episode Num: 375 Reward: 2360.5423084791482\n",
      "Total Timesteps: 353604 Episode Num: 376 Reward: 2493.8002104189104\n",
      "Total Timesteps: 354604 Episode Num: 377 Reward: 488.52025392632135\n",
      "Total Timesteps: 355604 Episode Num: 378 Reward: 574.5820849159458\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2362.991580\n",
      "---------------------------------------\n",
      "Total Timesteps: 356604 Episode Num: 379 Reward: 2436.0129765590564\n",
      "Total Timesteps: 357086 Episode Num: 380 Reward: 954.9371172954653\n",
      "Total Timesteps: 358086 Episode Num: 381 Reward: 904.228942996871\n",
      "Total Timesteps: 359086 Episode Num: 382 Reward: 338.7243719536295\n",
      "Total Timesteps: 360086 Episode Num: 383 Reward: 973.7736534328213\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 452.217980\n",
      "---------------------------------------\n",
      "Total Timesteps: 361086 Episode Num: 384 Reward: 267.8834097379431\n",
      "Total Timesteps: 362086 Episode Num: 385 Reward: 405.7632942719064\n",
      "Total Timesteps: 363086 Episode Num: 386 Reward: 397.90053579974744\n",
      "Total Timesteps: 364086 Episode Num: 387 Reward: 356.8957059999896\n",
      "Total Timesteps: 365086 Episode Num: 388 Reward: 478.207466963581\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 473.427557\n",
      "---------------------------------------\n",
      "Total Timesteps: 366086 Episode Num: 389 Reward: 544.486853546404\n",
      "Total Timesteps: 367086 Episode Num: 390 Reward: 548.0593995224056\n",
      "Total Timesteps: 368086 Episode Num: 391 Reward: 1153.6642713033086\n",
      "Total Timesteps: 369086 Episode Num: 392 Reward: 1012.4266668626082\n",
      "Total Timesteps: 370086 Episode Num: 393 Reward: 543.9033793161909\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 697.750040\n",
      "---------------------------------------\n",
      "Total Timesteps: 371086 Episode Num: 394 Reward: 610.2757428359363\n",
      "Total Timesteps: 372086 Episode Num: 395 Reward: 698.3197824141105\n",
      "Total Timesteps: 373086 Episode Num: 396 Reward: 830.8903035605476\n",
      "Total Timesteps: 374086 Episode Num: 397 Reward: 1099.374431837739\n",
      "Total Timesteps: 375086 Episode Num: 398 Reward: 1642.950070033396\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1811.137818\n",
      "---------------------------------------\n",
      "Total Timesteps: 376086 Episode Num: 399 Reward: 2018.81454992057\n",
      "Total Timesteps: 377086 Episode Num: 400 Reward: 1340.1634420431453\n",
      "Total Timesteps: 378086 Episode Num: 401 Reward: 908.094000738811\n",
      "Total Timesteps: 379086 Episode Num: 402 Reward: 1770.946920114277\n",
      "Total Timesteps: 380086 Episode Num: 403 Reward: 1541.2648254766555\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1481.611893\n",
      "---------------------------------------\n",
      "Total Timesteps: 381086 Episode Num: 404 Reward: 547.8246691236515\n",
      "Total Timesteps: 382086 Episode Num: 405 Reward: 454.81267752390545\n",
      "Total Timesteps: 383086 Episode Num: 406 Reward: 647.495108489372\n",
      "Total Timesteps: 384086 Episode Num: 407 Reward: 1498.8256681427213\n",
      "Total Timesteps: 385086 Episode Num: 408 Reward: 913.8840342480801\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1798.230526\n",
      "---------------------------------------\n",
      "Total Timesteps: 386086 Episode Num: 409 Reward: 1885.5276424322303\n",
      "Total Timesteps: 387086 Episode Num: 410 Reward: 2018.7332659102092\n",
      "Total Timesteps: 388086 Episode Num: 411 Reward: 1569.3872782945152\n",
      "Total Timesteps: 389086 Episode Num: 412 Reward: 1251.6143918388343\n",
      "Total Timesteps: 390086 Episode Num: 413 Reward: 1978.2633550992468\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1626.624355\n",
      "---------------------------------------\n",
      "Total Timesteps: 391086 Episode Num: 414 Reward: 1707.9556678268436\n",
      "Total Timesteps: 392086 Episode Num: 415 Reward: 1749.6033437008982\n",
      "Total Timesteps: 393086 Episode Num: 416 Reward: 1592.0143761519346\n",
      "Total Timesteps: 394086 Episode Num: 417 Reward: 2190.0488549792217\n",
      "Total Timesteps: 395086 Episode Num: 418 Reward: 2038.5705435260975\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2151.656433\n",
      "---------------------------------------\n",
      "Total Timesteps: 396086 Episode Num: 419 Reward: 2143.698817191924\n",
      "Total Timesteps: 397086 Episode Num: 420 Reward: 2307.116350986033\n",
      "Total Timesteps: 398086 Episode Num: 421 Reward: 2530.5247011669794\n",
      "Total Timesteps: 399086 Episode Num: 422 Reward: 2472.162319544898\n",
      "Total Timesteps: 400086 Episode Num: 423 Reward: 2410.25020137378\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2212.538618\n",
      "---------------------------------------\n",
      "Total Timesteps: 401086 Episode Num: 424 Reward: 2147.5948543373393\n",
      "Total Timesteps: 402086 Episode Num: 425 Reward: 2173.6464595537045\n",
      "Total Timesteps: 403086 Episode Num: 426 Reward: 2333.216645170101\n",
      "Total Timesteps: 404086 Episode Num: 427 Reward: 2468.076077627359\n",
      "Total Timesteps: 405086 Episode Num: 428 Reward: 2561.042457407655\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2609.490726\n",
      "---------------------------------------\n",
      "Total Timesteps: 406086 Episode Num: 429 Reward: 2497.369471109478\n",
      "Total Timesteps: 407086 Episode Num: 430 Reward: 2459.324481899323\n",
      "Total Timesteps: 408086 Episode Num: 431 Reward: 2598.7952332057944\n",
      "Total Timesteps: 409086 Episode Num: 432 Reward: 2560.1106235405177\n",
      "Total Timesteps: 410086 Episode Num: 433 Reward: 2600.8096643377203\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2618.688351\n",
      "---------------------------------------\n",
      "Total Timesteps: 411086 Episode Num: 434 Reward: 2491.8606473221653\n",
      "Total Timesteps: 412086 Episode Num: 435 Reward: 2475.892784157724\n",
      "Total Timesteps: 413086 Episode Num: 436 Reward: 2358.3124187640906\n",
      "Total Timesteps: 414086 Episode Num: 437 Reward: 2610.53280689888\n",
      "Total Timesteps: 415086 Episode Num: 438 Reward: 2617.1097349009656\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2353.848627\n",
      "---------------------------------------\n",
      "Total Timesteps: 416086 Episode Num: 439 Reward: 2394.924064420147\n",
      "Total Timesteps: 417086 Episode Num: 440 Reward: 2611.038209766147\n",
      "Total Timesteps: 418086 Episode Num: 441 Reward: 2737.18521133592\n",
      "Total Timesteps: 419086 Episode Num: 442 Reward: 2545.728448114198\n",
      "Total Timesteps: 420086 Episode Num: 443 Reward: 2370.9812445352295\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2673.989840\n",
      "---------------------------------------\n",
      "Total Timesteps: 421086 Episode Num: 444 Reward: 2644.629010617052\n",
      "Total Timesteps: 422086 Episode Num: 445 Reward: 2536.3490247604536\n",
      "Total Timesteps: 423086 Episode Num: 446 Reward: 2460.3835175573313\n",
      "Total Timesteps: 424086 Episode Num: 447 Reward: 2768.016592032662\n",
      "Total Timesteps: 425086 Episode Num: 448 Reward: 2664.489324947847\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2707.982085\n",
      "---------------------------------------\n",
      "Total Timesteps: 426086 Episode Num: 449 Reward: 2639.102864817791\n",
      "Total Timesteps: 427086 Episode Num: 450 Reward: 2650.507279235128\n",
      "Total Timesteps: 428086 Episode Num: 451 Reward: 2457.5287943334897\n",
      "Total Timesteps: 429086 Episode Num: 452 Reward: 2412.333472124904\n",
      "Total Timesteps: 430086 Episode Num: 453 Reward: 2755.886934995201\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2292.022950\n",
      "---------------------------------------\n",
      "Total Timesteps: 431086 Episode Num: 454 Reward: 2243.1108600442153\n",
      "Total Timesteps: 432086 Episode Num: 455 Reward: 2540.5104434053983\n",
      "Total Timesteps: 433086 Episode Num: 456 Reward: 2693.33049488706\n",
      "Total Timesteps: 434086 Episode Num: 457 Reward: 2573.000811400915\n",
      "Total Timesteps: 435086 Episode Num: 458 Reward: 2712.621147539019\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2756.732416\n",
      "---------------------------------------\n",
      "Total Timesteps: 436086 Episode Num: 459 Reward: 2757.5094442005\n",
      "Total Timesteps: 437086 Episode Num: 460 Reward: 2710.2178391915645\n",
      "Total Timesteps: 438086 Episode Num: 461 Reward: 2686.809337321835\n",
      "Total Timesteps: 439086 Episode Num: 462 Reward: 2600.687920825506\n",
      "Total Timesteps: 440086 Episode Num: 463 Reward: 2621.751986328853\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2632.870523\n",
      "---------------------------------------\n",
      "Total Timesteps: 441086 Episode Num: 464 Reward: 2684.6003583998922\n",
      "Total Timesteps: 442086 Episode Num: 465 Reward: 2772.300046034356\n",
      "Total Timesteps: 443086 Episode Num: 466 Reward: 2614.6986439765865\n",
      "Total Timesteps: 444086 Episode Num: 467 Reward: 2718.1593871288733\n",
      "Total Timesteps: 445086 Episode Num: 468 Reward: 2671.8274924293733\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2770.723834\n",
      "---------------------------------------\n",
      "Total Timesteps: 446086 Episode Num: 469 Reward: 2634.989571448836\n",
      "Total Timesteps: 447086 Episode Num: 470 Reward: 2811.0274915683513\n",
      "Total Timesteps: 448086 Episode Num: 471 Reward: 2711.995462415418\n",
      "Total Timesteps: 449086 Episode Num: 472 Reward: 2742.8929935186006\n",
      "Total Timesteps: 450086 Episode Num: 473 Reward: 2710.195499678412\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2642.398080\n",
      "---------------------------------------\n",
      "Total Timesteps: 451086 Episode Num: 474 Reward: 2810.6090804177297\n",
      "Total Timesteps: 452086 Episode Num: 475 Reward: 2653.328754417537\n",
      "Total Timesteps: 453086 Episode Num: 476 Reward: 2707.834948653486\n",
      "Total Timesteps: 454086 Episode Num: 477 Reward: 2518.6960547273166\n",
      "Total Timesteps: 455086 Episode Num: 478 Reward: 2786.283299356154\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2791.081998\n",
      "---------------------------------------\n",
      "Total Timesteps: 456086 Episode Num: 479 Reward: 2707.951382602293\n",
      "Total Timesteps: 457086 Episode Num: 480 Reward: 2766.798565567118\n",
      "Total Timesteps: 458086 Episode Num: 481 Reward: 2661.1908703222475\n",
      "Total Timesteps: 459086 Episode Num: 482 Reward: 2539.143593639462\n",
      "Total Timesteps: 460086 Episode Num: 483 Reward: 2759.8516230334335\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2632.835334\n",
      "---------------------------------------\n",
      "Total Timesteps: 461086 Episode Num: 484 Reward: 2478.7190180369175\n",
      "Total Timesteps: 462086 Episode Num: 485 Reward: 2498.3743808169324\n",
      "Total Timesteps: 463086 Episode Num: 486 Reward: 2750.9493564945865\n",
      "Total Timesteps: 464086 Episode Num: 487 Reward: 2677.679268111829\n",
      "Total Timesteps: 465086 Episode Num: 488 Reward: 2782.6299942886462\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2811.665949\n",
      "---------------------------------------\n",
      "Total Timesteps: 466086 Episode Num: 489 Reward: 2800.794217596815\n",
      "Total Timesteps: 467086 Episode Num: 490 Reward: 2639.67862156534\n",
      "Total Timesteps: 468086 Episode Num: 491 Reward: 2609.5827141129303\n",
      "Total Timesteps: 469086 Episode Num: 492 Reward: 2712.784485907987\n",
      "Total Timesteps: 470086 Episode Num: 493 Reward: 2514.87998604722\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2666.928362\n",
      "---------------------------------------\n",
      "Total Timesteps: 471086 Episode Num: 494 Reward: 2595.5757045744904\n",
      "Total Timesteps: 472086 Episode Num: 495 Reward: 2796.0978701568674\n",
      "Total Timesteps: 473086 Episode Num: 496 Reward: 2677.4351986173833\n",
      "Total Timesteps: 474086 Episode Num: 497 Reward: 2662.1646740110655\n",
      "Total Timesteps: 475086 Episode Num: 498 Reward: 2525.1866553151463\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2784.907735\n",
      "---------------------------------------\n",
      "Total Timesteps: 476086 Episode Num: 499 Reward: 2769.2184914689597\n",
      "Total Timesteps: 477086 Episode Num: 500 Reward: 2503.994880307506\n",
      "Total Timesteps: 478086 Episode Num: 501 Reward: 2559.6707511895743\n",
      "Total Timesteps: 479086 Episode Num: 502 Reward: 2587.343961882138\n",
      "Total Timesteps: 480086 Episode Num: 503 Reward: 2711.1487042177446\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2603.974281\n",
      "---------------------------------------\n",
      "Total Timesteps: 481086 Episode Num: 504 Reward: 2547.3962052515244\n",
      "Total Timesteps: 482086 Episode Num: 505 Reward: 2738.2315742164974\n",
      "Total Timesteps: 483086 Episode Num: 506 Reward: 2666.6915158152715\n",
      "Total Timesteps: 484086 Episode Num: 507 Reward: 2531.5298813442914\n",
      "Total Timesteps: 485086 Episode Num: 508 Reward: 2456.6926600342417\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2361.635102\n",
      "---------------------------------------\n",
      "Total Timesteps: 486086 Episode Num: 509 Reward: 2313.389568115038\n",
      "Total Timesteps: 487086 Episode Num: 510 Reward: 2239.740795895385\n",
      "Total Timesteps: 488086 Episode Num: 511 Reward: 2558.3636764560474\n",
      "Total Timesteps: 489086 Episode Num: 512 Reward: 2677.4861772963427\n",
      "Total Timesteps: 490086 Episode Num: 513 Reward: 2892.2302845464005\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2640.098380\n",
      "---------------------------------------\n",
      "Total Timesteps: 491086 Episode Num: 514 Reward: 2687.918397479231\n",
      "Total Timesteps: 492086 Episode Num: 515 Reward: 2617.522991562987\n",
      "Total Timesteps: 493086 Episode Num: 516 Reward: 2846.31029379394\n",
      "Total Timesteps: 494086 Episode Num: 517 Reward: 2897.809416432229\n",
      "Total Timesteps: 495086 Episode Num: 518 Reward: 2826.522503402192\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2700.957159\n",
      "---------------------------------------\n",
      "Total Timesteps: 496086 Episode Num: 519 Reward: 2726.885167116305\n",
      "Total Timesteps: 497086 Episode Num: 520 Reward: 2594.2667610312956\n",
      "Total Timesteps: 498086 Episode Num: 521 Reward: 2679.5610077962356\n",
      "Total Timesteps: 499086 Episode Num: 522 Reward: 2685.79268141005\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2790.169044\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We start the main loop over 500,000 timesteps\n",
    "while total_timesteps < max_timesteps:\n",
    "  \n",
    "  # If the episode is done\n",
    "  if done:\n",
    "\n",
    "    # If we are not at the very beginning, we start the training process of the model\n",
    "    if total_timesteps != 0:\n",
    "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
    "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "\n",
    "    # We evaluate the episode and we save the policy\n",
    "    if timesteps_since_eval >= eval_freq:\n",
    "      timesteps_since_eval %= eval_freq\n",
    "      evaluations.append(evaluate_policy(env, policy))\n",
    "      policy.save(file_name, directory=\"./models\")\n",
    "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "    \n",
    "    # When the training step is done, we reset the state of the environment\n",
    "    obs = env.reset()\n",
    "    \n",
    "    \n",
    "    # Set the Done to False\n",
    "    done = False\n",
    "    \n",
    "    # Set rewards and episode timesteps to zero\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "  \n",
    "  # Before 10000 timesteps, we play random actions\n",
    "  if total_timesteps < start_timesteps:\n",
    "    action = env.action_space.sample()\n",
    "  else: # After 10000 timesteps, we switch to the model\n",
    "    action = policy.select_action(np.array(obs))\n",
    "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "    if expl_noise != 0:\n",
    "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "  \n",
    "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "  new_obs, reward, done, _ = env.step(action)\n",
    "  \n",
    "  # We check if the episode is done\n",
    "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
    "  \n",
    "  # We increase the total reward\n",
    "  episode_reward += reward\n",
    "  \n",
    "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
    "\n",
    "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
    "  obs = new_obs\n",
    "  episode_timesteps += 1\n",
    "  total_timesteps += 1\n",
    "  timesteps_since_eval += 1\n",
    "\n",
    "# We add the last policy evaluation to our list of evaluations and we save our model\n",
    "evaluations.append(evaluate_policy(env, policy))\n",
    "if save_models: policy.save(\"%s\" % (file_name), directory=\"./models\")\n",
    "np.save(\"./results/%s\" % (file_name), evaluations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T21:24:07.869468600Z",
     "start_time": "2023-09-30T19:39:51.109885Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T21:24:07.892483800Z",
     "start_time": "2023-09-30T21:24:07.869468600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T21:24:07.900483400Z",
     "start_time": "2023-09-30T21:24:07.884483100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T21:24:07.916490600Z",
     "start_time": "2023-09-30T21:24:07.900483400Z"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
